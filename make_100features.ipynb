{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GWnHXLPfZrA_"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd /content/drive/MyDrive/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xd4lx1GWdM1t"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import glob\n",
        "import torch\n",
        "import torchaudio\n",
        "import torchaudio.transforms as T\n",
        "import librosa\n",
        "import IPython.display as ipd\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_healthy_paths = list(glob.glob('./SVD/train/healthy/*.wav'))\n",
        "train_pathology_paths = list(glob.glob('./SVD/train/pathology/*.wav'))\n",
        "test_healthy_paths = list(glob.glob('./SVD/test/healthy/*.wav'))\n",
        "test_pathology_paths = list(glob.glob('./SVD/test/pathology/*.wav'))\n",
        "print(f'train healthy   : {len(train_healthy_paths)} audios')\n",
        "print(f'train_pathology : {len(train_pathology_paths)} audios')\n",
        "print(f'test_healthy    : {len(test_healthy_paths)} audios')\n",
        "print(f'test_pathology  : {len(test_pathology_paths)} audios')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5mhxiMAlbhea"
      },
      "source": [
        "### Class extract features jitter, shimmer, etc... from wav file\n",
        "- ref: https://github.com/hyyoka/Acoustic-Features/blob/main/praat_features.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fxAV2z1SARcI"
      },
      "outputs": [],
      "source": [
        "# need installlation to define class below\n",
        "!pip install textgrid praat-parselmouth\n",
        "\n",
        "import parselmouth\n",
        "from parselmouth import Sound\n",
        "from parselmouth.praat import call\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "class FeatureGenerator:\n",
        "    def __init__(self, sound): # sound = Sound(wav_file)\n",
        "        self.sample_rate = 50000\n",
        "        self.fft_size = 512\n",
        "        self.window_size = 1024\n",
        "        self.hop_size = 512\n",
        "        self.time_step = self.hop_size/self.sample_rate\n",
        "        self.call = parselmouth.praat.call\n",
        "        self.high_resolution_desired = False\n",
        "\n",
        "\n",
        "    def _get_wv_feats(self, sound):\n",
        "        pre_emphasis = call(sound, 'Filter (pre-emphasis)', 80)\n",
        "        spectrum = call(pre_emphasis, 'To Spectrum', 'yes')\n",
        "        cog = call(spectrum, 'Get centre of gravity', 2) # Center of gravity'\n",
        "        std = call(spectrum, 'Get standard deviation', 2)\n",
        "        skw = call(spectrum, 'Get skewness', 2)\n",
        "        kur = call(spectrum, 'Get kurtosis', 2)\n",
        "        return {'KUR':kur,'SkW':skw,'COG':cog,'SD':std}\n",
        "\n",
        "    def _get_mfcc(self, sound, start, end):\n",
        "        \"\"\"mfcc extract: pre-emphasis => window => DFT => MelFilterBank => log => IDFT \"\"\"\n",
        "        mfcc = sound.to_mfcc(number_of_coefficients=12, time_step=self.time_step, window_length=self.window_size/self.sample_rate, maximum_frequency=7600)\n",
        "        mfcc_arr = mfcc.to_array()[1:] # 0 index is the energy of cepstrum\n",
        "        mfcc_bins = mfcc.x_bins()[:, 0]\n",
        "\n",
        "        def to_frame(time):\n",
        "            frame = np.searchsorted(mfcc_bins, time)-1\n",
        "            return frame if frame >= 0 else 0\n",
        "\n",
        "        start = to_frame(start)\n",
        "        end = to_frame(end)\n",
        "\n",
        "        _mfcc = mfcc_arr[:, start:end+1]\n",
        "        _mfcc = np.mean(_mfcc, axis=-1)\n",
        "        return _mfcc\n",
        "\n",
        "    def _get_formant(self, sound, start, end, get_part = False):\n",
        "        \"\"\"\n",
        "        formant extract\n",
        "        get_part: extraction of one third of the center\n",
        "        \"\"\"\n",
        "        formant = sound.to_formant_burg(time_step=self.time_step, max_number_of_formants=4.5, maximum_formant=4700.0, window_length=self.window_size/self.sample_rate, pre_emphasis_from=50)\n",
        "        duration = end - start\n",
        "        _formant = {}\n",
        "        for f in range(3):  # f1~f3\n",
        "            formant_ls = []\n",
        "            for i in range(3,100,3): # total 33 sections\n",
        "                formant_ls.append(formant.get_value_at_time(formant_number=f+1, time=start + i/100*duration))\n",
        "            if get_part:\n",
        "                formant_ls = formant_ls[10:-11]\n",
        "            formant_name = \"f\"+str(f+1)\n",
        "            _formant[formant_name] = formant_ls\n",
        "        return _formant\n",
        "\n",
        "\n",
        "    def _get_pitch(self, sound, start, end, get_part = False):\n",
        "        \"\"\"f0 extract\"\"\"\n",
        "\n",
        "        duration = end - start\n",
        "        pitch = sound.to_pitch(time_step=self.time_step, pitch_floor=75.0, pitch_ceiling=600.0)\n",
        "\n",
        "        _ff = []\n",
        "        for i in range(3,100,3): # total 24 sections 8-8-8\n",
        "          _ff.append(pitch.get_value_at_time(time=start + i/100*duration))\n",
        "        if get_part:\n",
        "          _ff = _ff[10:-11]\n",
        "\n",
        "        return _ff\n",
        "\n",
        "    def _get_intensity(self, sound, start, end, get_part = False):\n",
        "        \"\"\"intensity extract\"\"\"\n",
        "\n",
        "        duration = end - start\n",
        "        intensity = sound.to_intensity(minimum_pitch=100.0, time_step=self.time_step, subtract_mean=True)\n",
        "        _int = []\n",
        "        for i in range(3,100,3):\n",
        "            _int.append(intensity.get_value(time=start + i/100*duration))\n",
        "        if get_part:\n",
        "          _int = _int[10:-11]\n",
        "\n",
        "        return _int\n",
        "\n",
        "    def _get_jitter(self, sound, start, end):\n",
        "        \"\"\"jitter extract\"\"\"\n",
        "        point_process = self.call(sound, \"To PointProcess (periodic, cc)\", 75, 600)  # pitch_floor=75, pitch_ceiling=600\n",
        "        local_jitter = self.call(point_process, \"Get jitter (local)\", start, end, 0.0001, 0.02, 1.3)\n",
        "        localabsolute_jitter = self.call(point_process, \"Get jitter (local, absolute)\", start, end, 0.0001, 0.02, 1.3)\n",
        "        rap_jitter = self.call(point_process, \"Get jitter (rap)\", start, end, 0.0001, 0.02, 1.3)\n",
        "        ppq5_jitter = self.call(point_process, \"Get jitter (ppq5)\", start, end, 0.0001, 0.02, 1.3)\n",
        "        ddp_jitter = self.call(point_process, \"Get jitter (ddp)\", start, end, 0.0001, 0.02, 1.3)\n",
        "\n",
        "        _jitter = [local_jitter, localabsolute_jitter, rap_jitter, ppq5_jitter, ddp_jitter]\n",
        "\n",
        "        return _jitter\n",
        "\n",
        "    def _get_shimmer(self, sound, start, end):\n",
        "        \"\"\"shimmer extract\"\"\"\n",
        "        point_process = self.call(sound, \"To PointProcess (periodic, cc)\", 75, 600)  # pitch_floor=75, pitch_ceiling=600\n",
        "        local_shimmer = self.call([sound, point_process], \"Get shimmer (local)\", start, end, 0.0001, 0.02, 1.3, 1.6)\n",
        "        localdb_shimmer = self.call([sound, point_process], \"Get shimmer (local_dB)\", start, end, 0.0001, 0.02, 1.3, 1.6)\n",
        "        apq3_shimmer = self.call([sound, point_process], \"Get shimmer (apq3)\", start, end, 0.0001, 0.02, 1.3, 1.6)\n",
        "        apq5_shimmer = self.call([sound, point_process], \"Get shimmer (apq5)\", start, end, 0.0001, 0.02, 1.3, 1.6)\n",
        "        apq11_shimmer = self.call([sound, point_process], \"Get shimmer (apq11)\", start, end, 0.0001, 0.02, 1.3, 1.6)\n",
        "        dda_shimmer = self.call([sound, point_process], \"Get shimmer (dda)\", start, end, 0.0001, 0.02, 1.3, 1.6)\n",
        "        _shimmer = [local_shimmer, localdb_shimmer, apq3_shimmer, apq5_shimmer, apq11_shimmer, dda_shimmer]\n",
        "\n",
        "        return _shimmer\n",
        "\n",
        "    def _get_hnr(self, sound, start, end, method='cc', get_part=True):\n",
        "        \"\"\"\n",
        "        Calculate Harmonics-to-Noise Ratio (HNR); represents the degree of acoustic periodicity and Voice quality\n",
        "        \"\"\"\n",
        "        if method == 'ac':\n",
        "            hnr= sound.to_harmonicity_ac(time_step=self.time_step) # cross-correlation method (preferred).\n",
        "        else:\n",
        "            hnr= sound.to_harmonicity_cc(time_step=self.time_step) # cross-correlation method (preferred).\n",
        "\n",
        "        duration = end - start\n",
        "        _hnr= []\n",
        "\n",
        "        for i in range(3,100,3):\n",
        "            _hnr.append(hnr.get_value(time=start + i/100*duration))\n",
        "        if get_part:\n",
        "          _hnr = _hnr[10:-11]\n",
        "        _hnr = [h for h in _hnr if h != -200]\n",
        "        return _hnr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "nJjbFZrNWaQv"
      },
      "outputs": [],
      "source": [
        "!pip install textgrid praat-parselmouth\n",
        "\n",
        "import parselmouth\n",
        "from parselmouth.praat import call\n",
        "\n",
        "class FeatureGenerator:\n",
        "    def __init__(self, sound):  # sound = Sound(wav_file)\n",
        "        self.sample_rate = 50000\n",
        "        self.fft_size = 2048\n",
        "        self.window_size = 1200\n",
        "        self.hop_size = 600\n",
        "        self.time_step = self.hop_size / self.sample_rate\n",
        "        self.call = parselmouth.praat.call\n",
        "        self.high_resolution_desired = False\n",
        "\n",
        "    def _get_jitter(self, sound, start, end):\n",
        "        \"\"\"extract jitter\"\"\"\n",
        "        point_process = self.call(sound, \"To PointProcess (periodic, cc)\", 75, 600)  # pitch_floor=75, pitch_ceiling=600\n",
        "        local_jitter = self.call(point_process, \"Get jitter (local)\", start, end, 0.0001, 0.02, 1.3)\n",
        "        localabsolute_jitter = self.call(point_process, \"Get jitter (local, absolute)\", start, end, 0.0001, 0.02, 1.3)\n",
        "        rap_jitter = self.call(point_process, \"Get jitter (rap)\", start, end, 0.0001, 0.02, 1.3)\n",
        "        ppq5_jitter = self.call(point_process, \"Get jitter (ppq5)\", start, end, 0.0001, 0.02, 1.3)\n",
        "        ddp_jitter = self.call(point_process, \"Get jitter (ddp)\", start, end, 0.0001, 0.02, 1.3)\n",
        "\n",
        "        _jitter = [local_jitter, localabsolute_jitter, rap_jitter, ppq5_jitter, ddp_jitter]\n",
        "\n",
        "        return _jitter\n",
        "\n",
        "    def _get_shimmer(self, sound, start, end):\n",
        "        \"\"\"extract shimmer\"\"\"\n",
        "        point_process = self.call(sound, \"To PointProcess (periodic, cc)\", 75, 600)  # pitch_floor=75, pitch_ceiling=600\n",
        "        local_shimmer = self.call([sound, point_process], \"Get shimmer (local)\", start, end, 0.0001, 0.02, 1.3, 1.6)\n",
        "        localdb_shimmer = self.call([sound, point_process], \"Get shimmer (local_dB)\", start, end, 0.0001, 0.02, 1.3, 1.6)\n",
        "        apq3_shimmer = self.call([sound, point_process], \"Get shimmer (apq3)\", start, end, 0.0001, 0.02, 1.3, 1.6)\n",
        "        apq5_shimmer = self.call([sound, point_process], \"Get shimmer (apq5)\", start, end, 0.0001, 0.02, 1.3, 1.6)\n",
        "        apq11_shimmer = self.call([sound, point_process], \"Get shimmer (apq11)\", start, end, 0.0001, 0.02, 1.3, 1.6)\n",
        "        dda_shimmer = self.call([sound, point_process], \"Get shimmer (dda)\", start, end, 0.0001, 0.02, 1.3, 1.6)\n",
        "        _shimmer = [local_shimmer, localdb_shimmer, apq3_shimmer, apq5_shimmer, apq11_shimmer, dda_shimmer]\n",
        "\n",
        "        return _shimmer\n",
        "\n",
        "    def _get_jitter_shimmer(self, sound, start, end, num_intervals = 1):\n",
        "        # return dictionary\n",
        "        # Divide jitters and shimmers into num_intervals & save in one dict\n",
        "        duration = end - start # total speech length\n",
        "        interval_length = duration / num_intervals  # num_intervals equal parts\n",
        "        mat = []\n",
        "        for i in range(num_intervals):\n",
        "            interval_start = start + i * interval_length\n",
        "            interval_end = interval_start + interval_length\n",
        "            jitter_list = self._get_jitter(sound, interval_start, interval_end)\n",
        "            shimmer_list = self._get_shimmer(sound, interval_start, interval_end)\n",
        "            feature_list = jitter_list + shimmer_list\n",
        "            mat.append(feature_list)\n",
        "        df = pd.DataFrame(mat)\n",
        "        df_t = df.T\n",
        "        return df_t.values.tolist()\n",
        "\n",
        "def extract_features(path, n_intervals=10):\n",
        "    sound = Sound(path)\n",
        "    fg = FeatureGenerator(sound)\n",
        "    start_time = 0.0  # sec\n",
        "    end_time = sound.get_total_duration()  # sec\n",
        "    feature_list = fg._get_jitter_shimmer(sound, start_time, end_time, n_intervals)\n",
        "    words = [\"local_jitter_\", \"abs_jitter_\", \"rap_jitter_\", \"ppq5_\", \"ddp_\",\n",
        "              \"local_shimmer_\", \"abs_shimmer_\", \"apq3_\", \"apq5_\", \"dda_\"]\n",
        "    features ={} # dictionary contains instances\n",
        "    for i in range(len(words)):\n",
        "        for j in range(n_intervals):\n",
        "            features[words[i]+str(j)] = feature_list[i][j]\n",
        "    return features\n",
        "# create Dataframe function\n",
        "def create_dataframe(paths, label):\n",
        "    data = []\n",
        "    for file_path in paths:\n",
        "        features = extract_features(file_path)\n",
        "        features[\"IsPathology\"] = label\n",
        "        data.append(features)\n",
        "    return pd.DataFrame(data)\n",
        "# make Train dataset\n",
        "train_healthy_df = create_dataframe(train_healthy_paths, 0)\n",
        "train_pathology_df = create_dataframe(train_pathology_paths, 1)\n",
        "train_df = pd.concat([train_healthy_df, train_pathology_df], ignore_index=True)\n",
        "\n",
        "# make test dataset\n",
        "test_healthy_df = create_dataframe(test_healthy_paths, 0)\n",
        "test_pathology_df = create_dataframe(test_pathology_paths, 1)\n",
        "test_df = pd.concat([test_healthy_df, test_pathology_df], ignore_index=True)\n",
        "\n",
        "# Datasets to csv file (save)\n",
        "train_df.to_csv('train_100features.csv', index=False)\n",
        "test_df.to_csv('test_100features.csv', index=False)\n",
        "\n",
        "print(\"Train and test 100feature csvfiles have been saved.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qCAKAPFwS5CY"
      },
      "source": [
        "## Analysis through Sci-kit Learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uDAMPtQfS4VZ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd #\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "# use 7 classifier and finally ensemble them with voting(soft, hard both)\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "\n",
        "from sklearn.metrics import accuracy_score, f1_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T0Qv5cDCTVNR"
      },
      "outputs": [],
      "source": [
        "# load CSV\n",
        "train_df = pd.read_csv('train_100features.csv')\n",
        "test_df = pd.read_csv('test_100features.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DcGAIEvx2gOQ"
      },
      "outputs": [],
      "source": [
        "print((train_df.isnull().sum()))\n",
        "print((test_df.isnull().sum()))\n",
        "\n",
        "# check a lot of missing values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wb7fgYk_5p24"
      },
      "outputs": [],
      "source": [
        "from sklearn.impute import KNNImputer\n",
        "\n",
        "# imputation by KNN Imputer\n",
        "# It does not change the result significantly.\n",
        "imputer = KNNImputer(n_neighbors=5) # caution on parameter\n",
        "train_df[:] = imputer.fit_transform(train_df)\n",
        "test_df[:] = imputer.fit_transform(test_df)\n",
        "\n",
        "# check result(no missing values)\n",
        "print((train_df.isnull().sum()))\n",
        "print((test_df.isnull().sum()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qMKLxSnni7c6"
      },
      "outputs": [],
      "source": [
        "# Label\n",
        "# Seperate feature cols and Label col(IsPathology)\n",
        "X_train = train_df.drop('IsPathology', axis=1)\n",
        "y_train = train_df['IsPathology']\n",
        "X_test = test_df.drop('IsPathology', axis=1)\n",
        "y_test = test_df['IsPathology']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qFX-MTGCFZW8"
      },
      "outputs": [],
      "source": [
        "# Scaler\n",
        "# I've tried several scalers, but I think the basic standard is the best, so I annotate it.\n",
        "#from sklearn.preprocessing import MinMaxScaler, MaxAbsScaler, RobustScaler, Normalizer\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cS7b7vfS9cIr"
      },
      "outputs": [],
      "source": [
        "# Apply SMOTE 적용\n",
        "smote = SMOTE(random_state=42)\n",
        "X_train_scaled, y_train = smote.fit_resample(X_train_scaled, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "myRqMzepFjpy"
      },
      "source": [
        "### Adjust individual classifier parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kjq6LjBfGiil"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from scipy.stats import uniform, randint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q_nV-36bGdwu",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "#1.LogisticRegression\n",
        "import warnings\n",
        "from sklearn.exceptions import ConvergenceWarning\n",
        "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
        "\n",
        "\n",
        "log_reg_param_dist = {\n",
        "    'C': uniform(0.01, 10),\n",
        "    'solver': ['lbfgs', 'newton-cg', 'liblinear'],\n",
        "    'max_iter': [100, 200, 500]\n",
        "}\n",
        "\n",
        "log_reg_search = RandomizedSearchCV(LogisticRegression(random_state=42),\n",
        "                                    param_distributions=log_reg_param_dist,\n",
        "                                    n_iter=10, cv=5, scoring='accuracy', random_state=42)\n",
        "log_reg_search.fit(X_train_scaled, y_train)\n",
        "print('best score at train : ', round(log_reg_search.best_score_, 4))\n",
        "\n",
        "y_pred_log = log_reg_search.predict(X_test_scaled)\n",
        "log_accuracy = accuracy_score(y_test, y_pred_log)\n",
        "log_f1 = f1_score(y_test, y_pred_log)\n",
        "print(f'Logistic Regression Accuracy: {log_accuracy}, F1 Score: {log_f1}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a17X0UeAI6TC"
      },
      "outputs": [],
      "source": [
        "#2.RandomForestClassifier\n",
        "#rnd_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rnd_clf = RandomForestClassifier(n_estimators=10, max_depth=3, random_state=42)\n",
        "\n",
        "rf_param_dist = {\n",
        "    'n_estimators': randint(10, 100),\n",
        "    'max_depth': [None, 3, 10, 20, 30],\n",
        "    'min_samples_split': randint(2, 10),\n",
        "    'min_samples_leaf': randint(1, 10),\n",
        "    'max_features': ['sqrt', 'log2']\n",
        "}\n",
        "\n",
        "rf_search = RandomizedSearchCV(RandomForestClassifier(random_state=42),\n",
        "                               param_distributions=rf_param_dist,\n",
        "                               n_iter=10, cv=5, scoring='accuracy', random_state=42)\n",
        "\n",
        "\n",
        "rf_search.fit(X_train_scaled, y_train)\n",
        "print('best score at train : ', round(rf_search.best_score_, 4))\n",
        "\n",
        "y_pred_rf = rf_search.predict(X_test_scaled)\n",
        "rf_accuracy = accuracy_score(y_test, y_pred_rf)\n",
        "rf_f1 = f1_score(y_test, y_pred_rf)\n",
        "print(f'Random Forest Accuracy: {rf_accuracy}, F1 Score: {rf_f1}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YJa_HyIAKmyl"
      },
      "outputs": [],
      "source": [
        "#3.SupportVectorMachine(Classifier)\n",
        "#svm_clf = SVC(gamma=\"scale\", probability=True, random_state=42)\n",
        "#svm_clf = SVC(kernel='linear', gamma=10, C=1.0, probability=True, random_state=42)\n",
        "#svm_clf = SVC(kernel='rbf', gamma='auto', C=10.0, probability=True, random_state=42)\n",
        "#svm_clf = SVC(kernel='rbf', gamma='auto', C=1.0, class_weight={0: 1, 1: 2}, probability=True, random_state=42)\n",
        "svm_clf = SVC(gamma=10, probability=True, random_state=42)\n",
        "\n",
        "svm_param_dist = {\n",
        "    'C': uniform(0.1, 10),\n",
        "    'gamma': uniform(0.001, 1),\n",
        "    'kernel': ['linear', 'rbf', 'poly', 'sigmoid']\n",
        "}\n",
        "\n",
        "svm_search = RandomizedSearchCV(SVC(probability=True, random_state=42),\n",
        "                                param_distributions=svm_param_dist,\n",
        "                                n_iter=10, cv=5, scoring='accuracy', random_state=42)\n",
        "\n",
        "svm_search.fit(X_train_scaled, y_train)\n",
        "print('best score at train : ', round(svm_search.best_score_, 4))\n",
        "\n",
        "y_pred_svm = svm_search.predict(X_test_scaled)\n",
        "svm_accuracy = accuracy_score(y_test, y_pred_svm)\n",
        "svm_f1 = f1_score(y_test, y_pred_svm)\n",
        "print(f'Random Forest Accuracy: {svm_accuracy}, F1 Score: {svm_f1}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rXUwNNZrMYt0",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "#4.XGBoost Classifier\n",
        "from sklearn.exceptions import FitFailedWarning\n",
        "from sklearn.utils._testing import ignore_warnings\n",
        "# Ignore FitFailedWarning, UserWarning\n",
        "warnings.filterwarnings(\"ignore\", category=FitFailedWarning)\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "xgb_clf = XGBClassifier(n_estimators=100, max_depth=30, random_state=42)\n",
        "\n",
        "xgb_param_dist = {\n",
        "    'n_estimators': randint(50, 200),\n",
        "    'max_depth': randint(3, 20),\n",
        "    'learning_rate': uniform(0.01, 0.3),\n",
        "    'subsample': uniform(0.5, 1.0),\n",
        "    'colsample_bytree': uniform(0.5, 1.0)\n",
        "}\n",
        "\n",
        "xgb_search = RandomizedSearchCV(XGBClassifier(random_state=42),\n",
        "                                param_distributions=xgb_param_dist,\n",
        "                                n_iter=10, cv=5, scoring='accuracy', random_state=42)\n",
        "\n",
        "xgb_search.fit(X_train_scaled, y_train)\n",
        "print('best score at trai : ', round(xgb_search.best_score_, 4))\n",
        "\n",
        "y_pred_xgb = xgb_search.predict(X_test_scaled)\n",
        "xgb_accuracy = accuracy_score(y_test, y_pred_xgb)\n",
        "xgb_f1 = f1_score(y_test, y_pred_xgb)\n",
        "print(f'Random Forest Accuracy: {xgb_accuracy}, F1 Score: {xgb_f1}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "amlTBFN0MnWb"
      },
      "outputs": [],
      "source": [
        "#5 Gaussian.Naive Bayes\n",
        "from scipy.stats import loguniform\n",
        "\n",
        "nb_param_dist = {\n",
        "    'var_smoothing': loguniform(1e-12, 1e-1)\n",
        "}\n",
        "\n",
        "nb_search = RandomizedSearchCV(GaussianNB(),\n",
        "                                param_distributions=nb_param_dist,\n",
        "                                n_iter=10, cv=5, scoring='accuracy', random_state=42)\n",
        "\n",
        "nb_search.fit(X_train_scaled, y_train)\n",
        "print('best score at trai : ', round(nb_search.best_score_, 4))\n",
        "\n",
        "y_pred_nb = nb_search.predict(X_test_scaled)\n",
        "nb_accuracy = accuracy_score(y_test, y_pred_nb)\n",
        "nb_f1 = f1_score(y_test, y_pred_nb)\n",
        "print(f'Naive Bayes model Accuracy: {nb_accuracy}, F1 Score: {nb_f1}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_T0vyiNqMqd6"
      },
      "outputs": [],
      "source": [
        "#6. AdaBoostClassifier\n",
        "adaboost_clf = AdaBoostClassifier(random_state=42)\n",
        "adaboost_param_dist = {\n",
        "    'n_estimators': randint(50, 200),\n",
        "    'learning_rate': uniform(0.01, 2.0),\n",
        "    'algorithm': ['SAMME', 'SAMME.R']\n",
        "}\n",
        "\n",
        "adaboost_search = RandomizedSearchCV(AdaBoostClassifier(random_state=42),\n",
        "                                     param_distributions=adaboost_param_dist,\n",
        "                                     n_iter=10, cv=5, scoring='accuracy', random_state=42)\n",
        "\n",
        "adaboost_search.fit(X_train_scaled, y_train)\n",
        "\n",
        "print('Best score at train:', round(adaboost_search.best_score_, 4))\n",
        "\n",
        "y_pred_adaboost = adaboost_search.predict(X_test_scaled)\n",
        "\n",
        "adaboost_accuracy = accuracy_score(y_test, y_pred_adaboost)\n",
        "adaboost_f1 = f1_score(y_test, y_pred_adaboost)\n",
        "print(f'AdaBoost Classifier Accuracy: {adaboost_accuracy}, F1 Score: {adaboost_f1}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e7f-AUdiM0Nh"
      },
      "outputs": [],
      "source": [
        "# This part causes errors, but does not matter to performance.\n",
        "'''#7.BaggingClassifier\n",
        "from sklearn.exceptions import FitFailedWarning\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "warnings.filterwarnings(\"ignore\", category=FitFailedWarning)\n",
        "\n",
        "bagging_clf = BaggingClassifier(estimator= log_reg_search, n_estimators=50, random_state=42)\n",
        "bagging_param_dist = {\n",
        "    'n_estimators': randint(10, 100),\n",
        "    'max_samples': uniform(0.5, 1.0),\n",
        "    'max_features': uniform(0.5, 1.0),\n",
        "    'bootstrap': [True, False],\n",
        "    'bootstrap_features': [True, False]\n",
        "}\n",
        "\n",
        "bagging_search = RandomizedSearchCV(BaggingClassifier(estimator=log_reg_search),\n",
        "                                    param_distributions=bagging_param_dist,\n",
        "                                    n_iter=10, cv=5, scoring='accuracy', random_state=42)\n",
        "\n",
        "bagging_search.fit(X_train_scaled, y_train)\n",
        "print('best score at train: ', round(bagging_search.best_score_, 4))\n",
        "\n",
        "y_pred_bagging = bagging_search.predict(X_test_scaled)\n",
        "bagging_accuracy = accuracy_score(y_test, y_pred_bagging)\n",
        "bagging_f1 = f1_score(y_test, y_pred_bagging)\n",
        "print(f'Bagging Classifier Accuracy: {bagging_accuracy}, F1 Score: {bagging_f1}')'''"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 앙상블"
      ],
      "metadata": {
        "id": "0v4Aq-jipGbT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZN-z45DUS_LS"
      },
      "outputs": [],
      "source": [
        "# Define multiple different classifiers to fit into an ensemble\n",
        "\n",
        "print('Performance of individual models')\n",
        "print(f'Logistic Regression Accuracy: {log_accuracy}, F1 Score: {log_f1}')\n",
        "print(f'Random Forest Accuracy: {rf_accuracy}, F1 Score: {rf_f1}')\n",
        "print(f'Support Vector Machine Accuracy: {svm_accuracy}, F1 Score: {svm_f1}')\n",
        "print(f'XGBoost Classifier Accuracy: {xgb_accuracy}, F1 Score: {xgb_f1}')\n",
        "print(f'Naive Bayes model Accuracy: {nb_accuracy}, F1 Score: {nb_f1}')\n",
        "print(f'adaboost Classifier Accuracy: {adaboost_accuracy}, F1 Score: {nb_f1}')\n",
        "#print(f'bagging Classifier Accuracy: {bagging_accuracy}, F1 Score: {bagging_f1}')\n",
        "print('-------------------------------------------------------------------')\n",
        "\n",
        "## Defining ensemble learning model (I've tried bagging, boosting, and voting, and I think voting is the best in my experience)\n",
        "voting_clf = VotingClassifier(\n",
        "    estimators=[('lr', log_reg_search.best_estimator_),\n",
        "                ('rf', rf_search.best_estimator_),\n",
        "                ('xgb', xgb_search.best_estimator_),\n",
        "                ('ada', adaboost_search.best_estimator_),\n",
        "                #('svc', svm_search.best_estimator_),\n",
        "                #('bagging', bagging_search.best_estimator_),\n",
        "                #('nb', nb_search.best_estimator_)\n",
        "                ],\n",
        "    voting='soft' # To hard voting, change to 'hard'\n",
        ")\n",
        "# train model\n",
        "voting_clf.fit(X_train_scaled, y_train)\n",
        "y_pred = voting_clf.predict(X_test_scaled)\n",
        "\n",
        "voting_accuracy = accuracy_score(y_test, y_pred)\n",
        "voting_f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "print('Performance of Voting model')\n",
        "print(f'Accuracy: {voting_accuracy}')\n",
        "print(f'F1 Score: {voting_f1}')\n",
        "\n",
        "\n",
        "print('-------------------------------------------------------------------')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}